{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这是一个很好的爬虫例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.basketball-reference.com/leagues/NBA_2014_games-october.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2014_games-november.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2014_games-december.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2014_games-january.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2014_games-february.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2014_games-march.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2014_games-april.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2014_games-may.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2014_games-june.html\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import  pandas as pd\n",
    "import  requests\n",
    "import  numpy as np\n",
    "from  urllib.request import  urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "#第一步爬数据\n",
    "'''\n",
    "1 ：设置 url\n",
    "'''\n",
    "url1 =\"https://www.basketball-reference.com/leagues/NBA_2014_games-\"\n",
    "'''\n",
    "2 ：装报头\n",
    "\n",
    "headers = {\"User-Agent\" :'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'}\n",
    "result = requests.get(url1 , headers = headers)\n",
    "#print(result.text)\n",
    "'''\n",
    "all_month = np.array(['october','november','december','january','february','march','april','may','june'])\n",
    "\n",
    "def get_content():\n",
    "    list = []\n",
    "    for i in range(len(all_month)):\n",
    "        #url = url1.format(month=all_month[i])\n",
    "        url = url1+all_month[i]+\".html\"\n",
    "        print(url)\n",
    "        html = urlopen(url).read()\n",
    "       # print(html)\n",
    "        bsObj = BeautifulSoup(html,'lxml')\n",
    "        #print(bsObj)# 就是原网页\n",
    "        rows = [dd for dd in bsObj.select('tbody tr')]#selectk()可以多重刷选 进行tbody 中的tr标签筛选（每个dd 中都是tr标签\n",
    "        #print(rows) #所有的tr 标签\n",
    "        for row in rows:\n",
    "            cell = [i.text for i in row.find_all('td')]#对于每一个tr标签内也可以进行td标签筛选 ，就是筛选信息了（有用的）\n",
    "            list.append(cell)\n",
    "       # print(list)\n",
    "    return list#返回二维列表\n",
    "#存储为scv格式\n",
    "def save():\n",
    "    file = open('D:\\\\resu\\\\python_test\\\\练习文件\\练习\\\\nba\\\\matches.csv','w',newline='')#地址要自己改\n",
    "    list = get_content()\n",
    "    df_data = pd.DataFrame(columns=[1,2,3,4,5,6,7,8,9] ,data=list) #行列\n",
    "    df_data.to_csv(file)\n",
    "    print('done')\n",
    "\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
